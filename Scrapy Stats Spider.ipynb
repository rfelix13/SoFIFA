{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> SoFIFA Player Stats</center>\n",
    "## <center>Using Scrapy and Jupyter Notebook to Download Player Stats from SoFIFA.com<center/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I used the scrapy tutorial from the docs (1) and the blog on JJ's world about using Scrapy in Jupyter notebook (2).\n",
    "<p>(1) <a href=https://doc.scrapy.org/en/latest/intro/tutorial.html>https://doc.scrapy.org/en/latest/intro/tutorial.html</a></p>\n",
    "<p>(2) <a href=https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html>https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html</a></p>\n",
    "\n",
    "Please make sure to run the URL Spider first! You need to have a sofi_urls.jl file saved in the current directory for this to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonStatsWriterPipeline(object):\n",
    "    \n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('sofi_stats.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Spider Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import logging\n",
    "\n",
    "class Sofi_Stats_Spider(scrapy.Spider):\n",
    "    \n",
    "    name='stats_sofi'\n",
    "    \n",
    "    # Load the Json file \n",
    "    data=[]\n",
    "    with open(\"sofi_urls.jl\", \"r\") as read_file:\n",
    "        for line in read_file:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    start_urls=[x['url'] for x in data] # use our links in the json file\n",
    "    \n",
    "    \n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL':logging.WARNING,\n",
    "        'ITEM_PIPELINES':{'__main__.JsonStatsWriterPipeline': 1},\n",
    "        #'FEED_FORMAT':'json',\n",
    "        #'FEED_URI':'sofi_stats.json',  # Uncomment these if you want json rather than jl files\n",
    "    }\n",
    "    \n",
    "    \n",
    "    def parse(self,response):\n",
    "        \n",
    "        sd={} # Create the dictionary\n",
    "        \n",
    "        # Name and SoFIFA ID\n",
    "        name,ID=response.css('.info h1::text').getall()[0].split('(')\n",
    "        name=name[:-1]\n",
    "        ID=ID[4:-2]\n",
    "        \n",
    "        sd['name']=name\n",
    "        sd['id']=ID\n",
    "        \n",
    "        # Full Name, Country, Position(s)\n",
    "        sd['full_name'] = response.css('.bp3-text-overflow-ellipsis::text').get()[:-1]\n",
    "        sd['country'] = response.css('.bp3-text-overflow-ellipsis a::attr(title)').get()\n",
    "        sd['positions'] = response.css('.meta.bp3-text-overflow-ellipsis span').re('>(.*)<')\n",
    "\n",
    "        # Age, DOB, Height, Weight\n",
    "        helper = response.css('.meta.bp3-text-overflow-ellipsis::text').getall()[-1].split()\n",
    "\n",
    "        sd['age']=int(helper[1])\n",
    "        dob=helper[2]+' '+helper[3]+' '+helper[4]\n",
    "        sd['dob']=dob[1:-1]\n",
    "\n",
    "        ht=helper[5]\n",
    "        ht=ht.split(ht[1])\n",
    "        ht[1]=ht[1].replace('\"','')\n",
    "        sd['height']=int(ht[0])*12+int(ht[1])\n",
    "\n",
    "\n",
    "        sd['weight']=int(helper[6][:3])\n",
    "    \n",
    "        # Overall Rating, Potential, Value, Wage\n",
    "\n",
    "        helper = response.css('.column.col-4.text-center span::text').getall()\n",
    "        for x in helper:\n",
    "            if x[0]=='+':\n",
    "                helper.remove(x)\n",
    "\n",
    "        sd['overall']=int(helper[0])\n",
    "        sd['potential']=int(helper[1])\n",
    "        sd['value'],sd['wage']=helper[2:4]\n",
    "        \n",
    "        # Preferred Foot, International Reputation, Weak Foot, Skill Moves\n",
    "\n",
    "        sd['preferred_foot'],int_rep,weak_foot,skill_moves = response.css('.column.col-6 ul.bp3-text-overflow-ellipsis.pl li::text').getall()[1:8:2]\n",
    "        sd['int_rep']=int(int_rep)\n",
    "        sd['weak_foot']=int(weak_foot)\n",
    "        sd['skill_moves']=int(skill_moves)\n",
    "        \n",
    "        # Work Rate, Body Type, Real Face, Release Clause\n",
    "        \n",
    "        helper=response.css('.column.col-6 ul.bp3-text-overflow-ellipsis.pl li span::text').getall()\n",
    "\n",
    "        sd['work_rate']=helper[0]\n",
    "        sd['body_type']=helper[1]\n",
    "        sd['real_face']=helper[2]\n",
    "        try:\n",
    "            sd['release_clause'] = helper[3]\n",
    "        except:\n",
    "            sd['release_clause']=None\n",
    "            \n",
    "        # Club, Club Rating, Club Position, Jersey Number\n",
    "\n",
    "        try: \n",
    "            sd['club']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li h6').re('\\\">(.*)</a>')[0]\n",
    "        except:\n",
    "            sd['club']=None\n",
    "\n",
    "        try:\n",
    "            sd['club_rating']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li span::text').getall()[0]\n",
    "            sd['club_position']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li span::text').getall()[1]\n",
    "            sd['jersey_number']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li::text').getall()[1]\n",
    "            #sd['joined']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li::text').getall()[2]\n",
    "            #sd['contract_expir']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li::text').getall()[3]\n",
    "        except:\n",
    "            sd['club_rating']=None\n",
    "            sd['club_position']=None\n",
    "            sd['jersey_number']=None \n",
    "            \n",
    "        # Country, country_rating, country_position, country_jersey\n",
    "        # NOT ALL PLAYERS PLAY FOR THE NATIONAL TEAM \n",
    "\n",
    "        try: \n",
    "            sd['national_team']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li h6').re('\\\">(.*)</a>')[1]\n",
    "        except: \n",
    "            sd['national_team']=None\n",
    "        try:\n",
    "            sd['nt_rating']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li span::text').getall()[2]\n",
    "        except:\n",
    "            sd['nt_rating']=None    \n",
    "        try:    \n",
    "            sd['nt_position']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li span::text').getall()[3]\n",
    "        except:\n",
    "            sd['nt_position']=None    \n",
    "        try:    \n",
    "            sd['nt_jersey']=response.css('.bp3-text-overflow-ellipsis.pl.text-right li::text').getall()[5]\n",
    "        except:\n",
    "            sd['nt_jersey']=None\n",
    "\n",
    "        # All Skill Attributes\n",
    "        \n",
    "        helper=response.css('ul li span::text').getall()\n",
    "  \n",
    "        # We need to remove any notices that the attribute has increased\n",
    "\n",
    "        for x in helper:\n",
    "            if x[0]=='+':\n",
    "                helper.remove(x)\n",
    "            elif x[0]=='-':\n",
    "                helper.remove(x)\n",
    "        \n",
    "        att_list=helper[(helper.index('Crossing')-1):]\n",
    "        \n",
    "        fixed_list = att_list[:51]\n",
    "        fixed_list.append('Composure')\n",
    "        fixed_list.extend(att_list[51:58])\n",
    "        fixed_list.append('GK Diving')\n",
    "        fixed_list.append(att_list[58])\n",
    "        fixed_list.append('GK Handling')\n",
    "        fixed_list.append(att_list[59])\n",
    "        fixed_list.append('GK Kicking')\n",
    "        fixed_list.append(att_list[60])\n",
    "        fixed_list.append('GK Positioning')\n",
    "        fixed_list.append(att_list[61])\n",
    "        fixed_list.append('GK Reflexes')\n",
    "        fixed_list.extend(att_list[62:])\n",
    "\n",
    "\n",
    "        for i in range(int((len(fixed_list[:68]))/2)):\n",
    "             sd[fixed_list[2*i+1].lower()]=int(fixed_list[2*i])\n",
    "                \n",
    "        sd['traits']=fixed_list[68:]\n",
    "        \n",
    "        yield sd\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Crawler for the Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-19 22:44:57 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2019-11-19 22:44:57 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Darwin-19.0.0-x86_64-i386-64bit\n",
      "2019-11-19 22:44:57 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36)'}\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({'USER_AGENT':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36)'})\n",
    "process.crawl(Sofi_Stats_Spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move the file to the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv sofi_stats.jl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
