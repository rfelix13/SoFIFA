{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>SoFIFA URL Spider</center>\n",
    "## <center>Using Scrapy and Jupyter Notebook to Download Player URL from SoFIFA.com</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I used the scrapy tutorial from the docs (1) and the blog on JJ's world about using Scrapy in Jupyter notebook (2).\n",
    "<p>(1) <a href=https://doc.scrapy.org/en/latest/intro/tutorial.html>https://doc.scrapy.org/en/latest/intro/tutorial.html</a></p>\n",
    "<p>(2) <a href=https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html>https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy \n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "class JsonURLWriterPipeline(object):\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.file = open('sofi_urls.jl', 'w')\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.file.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        line = json.dumps(dict(item)) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Spider Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class Sofi_URL_Spider(scrapy.Spider):\n",
    "    \n",
    "    name='url_sofi' \n",
    "    start_urls=['https://sofifa.com/players?r=200011&set=true']\n",
    "    \n",
    "    custom_settings = {\n",
    "        'LOG_LEVEL':logging.WARNING,\n",
    "        'ITEM_PIPELINES':{'__main__.JsonURLWriterPipeline': 1},\n",
    "        #'FEED_FORMAT':'json',\n",
    "        #'FEED_URI':'sofi_urls.json',   # Uncomment these if you want json rather than jl\n",
    "    }\n",
    "        \n",
    "    def parse(self,response):   # There's going to be two steps, we need all the player page urls first, then will crawl all of those pages  \n",
    "        \n",
    "        for x in response.css('a.nowrap::attr(href)').getall():\n",
    "            yield {'url': 'https://sofifa.com' + x}\n",
    "        \n",
    "        next_page = 'https://sofifa.com/players?r=200011&set=true'+response.css('.pagination a::attr(href)').getall()[-1]\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Crawler for the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-19 22:39:51 [scrapy.utils.log] INFO: Scrapy 1.8.0 started (bot: scrapybot)\n",
      "2019-11-19 22:39:51 [scrapy.utils.log] INFO: Versions: lxml 4.4.1.0, libxml2 2.9.9, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 19.10.0, Python 3.7.4 (default, Aug 13 2019, 15:17:50) - [Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 19.0.0 (OpenSSL 1.1.1c  28 May 2019), cryptography 2.7, Platform Darwin-19.0.0-x86_64-i386-64bit\n",
      "2019-11-19 22:39:51 [scrapy.crawler] INFO: Overridden settings: {'LOG_LEVEL': 30, 'USER_AGENT': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36)'}\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess({'USER_AGENT':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36)'})\n",
    "process.crawl(Sofi_URL_Spider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This little script will remove old player links that aren't currently in the game\n",
    "# For whatever reason, some old urls still get included in the scraping process\n",
    "\n",
    "url_list=[]\n",
    "with open('sofi_urls.jl') as urls:\n",
    "    for line in urls:\n",
    "        dic=dict(json.loads(line))\n",
    "        if '200011' in dic['url']:\n",
    "            url_list.append(dic)\n",
    "\n",
    "with open('sofi_urls.jl','w') as urls:\n",
    "    for x in url_list:\n",
    "        json.dump(x, urls)\n",
    "        urls.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv sofi_urls.jl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
